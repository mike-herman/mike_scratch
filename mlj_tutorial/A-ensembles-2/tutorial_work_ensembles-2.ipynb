{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to tutorial](https://juliaai.github.io/DataScienceTutorials.jl/getting-started/ensembles-2/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/Personal Stuff/Repos/mike_scratch/mlj_tutorial/A-ensembles-2`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The active manifest file has dependencies that were resolved with a different julia version (1.7.1). Unexpected behavior may occur.\n",
      "└ @ nothing /Users/michaelherman/Documents/Personal Stuff/Repos/mike_scratch/mlj_tutorial/A-ensembles-2/Manifest.toml:0\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to build a random forest regressor on the Boston data. We've done random forest and ensemble modeling in previous tutorials, so this is largely practice. I'm going to code it with minimal commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using PyPlot\n",
    "using PrettyPrinting\n",
    "using StableRNGs\n",
    "import DataFrames: DataFrame, describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats:\n",
      "Length:         506\n",
      "Missing Count:  0\n",
      "Mean:           22.532806\n",
      "Minimum:        5.000000\n",
      "1st Quartile:   17.025000\n",
      "Median:         21.200000\n",
      "3rd Quartile:   25.000000\n",
      "Maximum:        50.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:           Float64\n"
     ]
    }
   ],
   "source": [
    "X, y = @load_boston\n",
    "sch = schema(X)\n",
    "p = length(sch.names)\n",
    "describe(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /Users/michaelherman/.julia/packages/MLJModels/tMgLW/src/loading.jl:168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJDecisionTreeInterface ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJDecisionTreeInterface.DecisionTreeRegressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌───────────────────────────────────────────────────┬─────────────┬─────────────\n",
       "│\u001b[22m measure                                           \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m ⋯\n",
       "├───────────────────────────────────────────────────┼─────────────┼─────────────\n",
       "│ RootMeanSquaredError()                            │ 7.06        │ predict    ⋯\n",
       "│ RootMeanSquaredLogProportionalError(offset = 1.0) │ 0.328       │ predict    ⋯\n",
       "└───────────────────────────────────────────────────┴─────────────┴─────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = machine(DecisionTreeRegressor(), X, y)\n",
    "e = evaluate!(tree,  resampling=Holdout(fraction_train=0.8), measure=[rms, rmslp1])\n",
    "e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a single decision tree. For a random forest, we need a bagging ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeterministicEnsembleModel(\n",
       "    model = DecisionTreeRegressor(\n",
       "            max_depth = -1,\n",
       "            min_samples_leaf = 5,\n",
       "            min_samples_split = 2,\n",
       "            min_purity_increase = 0.0,\n",
       "            n_subfeatures = 3,\n",
       "            post_prune = false,\n",
       "            merge_purity_threshold = 1.0,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    atomic_weights = Float64[],\n",
       "    bagging_fraction = 0.8,\n",
       "    rng = Random._GLOBAL_RNG(),\n",
       "    n = 100,\n",
       "    acceleration = CPU1{Nothing}(nothing),\n",
       "    out_of_bag_measure = Any[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "forest = EnsembleModel(model=DecisionTreeRegressor(n_subfeatures=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{DeterministicTunedModel{Grid,…},…}.\n",
      "└ @ MLJBase /Users/michaelherman/.julia/packages/MLJBase/MuLnJ/src/machines.jl:464\n",
      "┌ Info: Attempting to evaluate 30 models.\n",
      "└ @ MLJTuning /Users/michaelherman/.julia/packages/MLJTuning/Al9yX/src/tuned_models.jl:680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:   0%[>                        ]  ETA: N/A\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:   3%[>                        ]  ETA: 0:01:55\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:   7%[=>                       ]  ETA: 0:01:07\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  10%[==>                      ]  ETA: 0:00:44\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  13%[===>                     ]  ETA: 0:00:33\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  17%[====>                    ]  ETA: 0:00:26\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  20%[=====>                   ]  ETA: 0:00:22\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  23%[=====>                   ]  ETA: 0:00:19\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  27%[======>                  ]  ETA: 0:00:17\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  30%[=======>                 ]  ETA: 0:00:16\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  33%[========>                ]  ETA: 0:00:14\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  37%[=========>               ]  ETA: 0:00:13\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  40%[==========>              ]  ETA: 0:00:12\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  43%[==========>              ]  ETA: 0:00:12\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  47%[===========>             ]  ETA: 0:00:11\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  50%[============>            ]  ETA: 0:00:11\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  53%[=============>           ]  ETA: 0:00:10\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  57%[==============>          ]  ETA: 0:00:09\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  60%[===============>         ]  ETA: 0:00:09\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  63%[===============>         ]  ETA: 0:00:08\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  67%[================>        ]  ETA: 0:00:08\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  70%[=================>       ]  ETA: 0:00:07\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  73%[==================>      ]  ETA: 0:00:07\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  77%[===================>     ]  ETA: 0:00:06\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  80%[====================>    ]  ETA: 0:00:05\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  83%[====================>    ]  ETA: 0:00:04\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  87%[=====================>   ]  ETA: 0:00:04\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  90%[======================>  ]  ETA: 0:00:03\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  93%[=======================> ]  ETA: 0:00:02\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels:  97%[========================>]  ETA: 0:00:01\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 30 metamodels: 100%[=========================] Time: 0:00:31\u001b[39m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "rng = StableRNG(71)\n",
    "m = machine(forest, X, y)\n",
    "r = range(forest, :n, lower=10, upper=1000)\n",
    "curves = learning_curve!(m, resampling=Holdout(fraction_train=0.8, rng=rng), range=r, measure=rms);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the learning curve will help us determine the ideal number of trees.\n",
    "\n",
    "But since plotting is still busted in this env, we'll just find the record with the minimum measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 727.0\n",
       "   3.7328328312671166"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hcat(curves.parameter_values, curves.measurements)[findmin(curves.measurements)[2],:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 727 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "forest.n = 727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(model = (max_depth = -1,\n",
      "          min_samples_leaf = 5,\n",
      "          min_samples_split = 2,\n",
      "          min_purity_increase = 0.0,\n",
      "          n_subfeatures = 3,\n",
      "          post_prune = false,\n",
      "          merge_purity_threshold = 1.0,\n",
      "          rng = Random._GLOBAL_RNG()),\n",
      " atomic_weights = [],\n",
      " bagging_fraction = 0.8,\n",
      " rng = Random._GLOBAL_RNG(),\n",
      " n = 727,\n",
      " acceleration = CPU1{Nothing}(nothing),\n",
      " out_of_bag_measure = [])"
     ]
    }
   ],
   "source": [
    "params(forest) |> pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌───────────────────────────────────────────────────┬─────────────┬─────────────\n",
       "│\u001b[22m measure                                           \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m ⋯\n",
       "├───────────────────────────────────────────────────┼─────────────┼─────────────\n",
       "│ RootMeanSquaredError()                            │ 3.99        │ predict    ⋯\n",
       "│ RootMeanSquaredLogProportionalError(offset = 1.0) │ 0.252       │ predict    ⋯\n",
       "└───────────────────────────────────────────────────┴─────────────┴─────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_sf = range(forest, :(model.n_subfeatures), lower=1, upper=12)\n",
    "r_bf = range(forest, :bagging_fraction, lower=0.4, upper=1.0);\n",
    "\n",
    "tuned_forest = TunedModel(model=forest,\n",
    "                          tuning=Grid(resolution=3),\n",
    "                          resampling=CV(nfolds=6, rng=StableRNG(71)),\n",
    "                          ranges=[r_sf,r_bf],\n",
    "                          measures=rms)\n",
    "m = machine(tuned_forest, X, y)\n",
    "e = evaluate!(m, resampling=Holdout(fraction_train=0.8),\n",
    "              measure=[rms, rmslp1])\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×3 Matrix{Any}:\n",
       " 12  0.7  4.10075\n",
       " 12  1.0  4.46944\n",
       "  1  1.0  4.71862\n",
       "  6  1.0  3.83574\n",
       "  6  0.4  4.18354\n",
       " 12  0.4  4.14608\n",
       "  1  0.7  4.98\n",
       "  6  0.7  3.92241\n",
       "  1  0.4  5.45518"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = report(m)\n",
    "res = r.plotting\n",
    "hcat(res.parameter_values[:,1:2], res.measurements)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum occurred at 6 subfeatures and a bagging fraction of 1.0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms(ŷ, y) = 2.407968274450449\n"
     ]
    }
   ],
   "source": [
    "ŷ = predict(m, X)    # y\\hat + tab => ŷ\n",
    "@show rms(ŷ, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
